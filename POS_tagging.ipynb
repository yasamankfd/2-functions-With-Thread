{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNgA2B7rgwS9OK3FhvNhGKj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yasamankfd/2-functions-With-Thread/blob/master/POS_tagging.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ZXNiS0ljOji",
        "outputId": "6a586540-061c-4fe6-b2eb-942c7a655e92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n"
          ]
        }
      ],
      "source": [
        "pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import treebank\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "nltk.download('treebank')\n",
        "nltk.download('universal_tagset')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RaPN7bYTjW1O",
        "outputId": "584d63fe-916e-4da7-be78-69c56096b612"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/treebank.zip.\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = treebank.tagged_sents(tagset='universal')\n",
        "train_data, test_data = train_test_split(corpus, test_size=0.25, random_state=0)"
      ],
      "metadata": {
        "id": "mmdlH7PfjaXt"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_hmm(train_data):\n",
        "    tag_counts = defaultdict(int)\n",
        "    transition_counts = defaultdict(lambda: defaultdict(int))\n",
        "    emission_counts = defaultdict(lambda: defaultdict(int))\n",
        "\n",
        "    for sent in train_data:\n",
        "        prev_tag = '<s>'\n",
        "        tag_counts[prev_tag] += 1\n",
        "        for word, tag in sent:\n",
        "            transition_counts[prev_tag][tag] += 1\n",
        "            emission_counts[tag][word] += 1\n",
        "            tag_counts[tag] += 1\n",
        "            prev_tag = tag\n",
        "        transition_counts[prev_tag]['</s>'] += 1\n",
        "        tag_counts['</s>'] += 1\n",
        "\n",
        "    transition_probs = {prev_tag: {tag: count / tag_counts[prev_tag] for tag, count in tags.items()}\n",
        "                        for prev_tag, tags in transition_counts.items()}\n",
        "    emission_probs = {tag: {word: count / tag_counts[tag] for word, count in words.items()}\n",
        "                      for tag, words in emission_counts.items()}\n",
        "\n",
        "    return transition_probs, emission_probs, tag_counts\n",
        "\n",
        "transition_probs, emission_probs, tag_counts = train_hmm(train_data)\n"
      ],
      "metadata": {
        "id": "g9Js_DOPkpkL"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def viterbi(sentence, tag_transition_probs, word_tag_probs, tag_counts):\n",
        "    V = [{}]\n",
        "    path = {}\n",
        "\n",
        "    for tag in tag_transition_probs['<s>']:\n",
        "        V[0][tag] = tag_transition_probs['<s>'].get(tag, 0) * word_tag_probs[tag].get(sentence[0], 0)\n",
        "        path[tag] = [tag]\n",
        "\n",
        "    for t in range(1, len(sentence)):\n",
        "        V.append({})\n",
        "        new_path = {}\n",
        "\n",
        "        for tag in tag_counts:\n",
        "            if tag in ['<s>', '</s>']:\n",
        "                continue\n",
        "            prob, state = max((V[t-1][prev_tag] * tag_transition_probs[prev_tag].get(tag, 0) * word_tag_probs[tag].get(sentence[t], 0), prev_tag)\n",
        "                              for prev_tag in V[t-1])\n",
        "            V[t][tag] = prob\n",
        "            new_path[tag] = path[state] + [tag]\n",
        "\n",
        "        path = new_path\n",
        "\n",
        "    n = len(sentence) - 1\n",
        "    prob, state = max((V[n][tag], tag) for tag in V[n])\n",
        "    return path[state]\n"
      ],
      "metadata": {
        "id": "TO8SC7lqkuRe"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def most_frequent_tag_baseline(train_data, test_data):\n",
        "    tag_fd = nltk.FreqDist(tag for sent in train_data for (word, tag) in sent)\n",
        "    most_freq_tag = tag_fd.max()\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for sent in test_data:\n",
        "        for (word, tag) in sent:\n",
        "            if tag == most_freq_tag:\n",
        "                correct += 1\n",
        "            total += 1\n",
        "    return correct / total\n"
      ],
      "metadata": {
        "id": "AS8zEjnIkxG_"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tag import hmm\n",
        "from nltk.tag import UnigramTagger\n",
        "\n",
        "hmm_trainer = hmm.HiddenMarkovModelTrainer()\n",
        "hmm_tagger = hmm_trainer.train(train_data)\n",
        "unigram_tagger = UnigramTagger(train_data)\n"
      ],
      "metadata": {
        "id": "KE8nunuBkzAA"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_tagger(tagger, test_data):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for sent in test_data:\n",
        "        words, true_tags = zip(*sent)\n",
        "        pred_tags = tagger(words)\n",
        "        correct += sum(t1 == t2 for t1, t2 in zip(true_tags, pred_tags))\n",
        "        total += len(true_tags)\n",
        "    return correct / total\n",
        "\n",
        "viterbi_accuracy = evaluate_tagger(lambda sent: viterbi(sent, transition_probs, emission_probs, tag_counts), test_data)\n",
        "most_frequent_tag_accuracy = most_frequent_tag_baseline(train_data, test_data)\n",
        "hmm_tagger_accuracy = hmm_tagger.evaluate(test_data)\n",
        "unigram_tagger_accuracy = unigram_tagger.evaluate(test_data)\n",
        "\n",
        "print(f\"Viterbi Algorithm Accuracy         : {viterbi_accuracy:.4f}\")\n",
        "print(f\"Most Frequent Tag Baseline Accuracy: {most_frequent_tag_accuracy:.4f}\")\n",
        "print(f\"NLTK HMM Tagger Accuracy           : {hmm_tagger_accuracy:.4f}\")\n",
        "print(f\"NLTK Unigram Tagger Accuracy       : {unigram_tagger_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZGdy_OH0k15g",
        "outputId": "7fc6fd87-145e-4349-87c4-ef66b98e0cf1"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-7949b0ab869f>:13: DeprecationWarning: \n",
            "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
            "  instead.\n",
            "  hmm_tagger_accuracy = hmm_tagger.evaluate(test_data)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Viterbi Algorithm Accuracy         : 0.3164\n",
            "Most Frequent Tag Baseline Accuracy: 0.2874\n",
            "NLTK HMM Tagger Accuracy           : 0.4490\n",
            "NLTK Unigram Tagger Accuracy       : 0.8923\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-7949b0ab869f>:14: DeprecationWarning: \n",
            "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
            "  instead.\n",
            "  unigram_tagger_accuracy = unigram_tagger.evaluate(test_data)\n"
          ]
        }
      ]
    }
  ]
}